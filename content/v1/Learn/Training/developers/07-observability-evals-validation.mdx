---
title: "Module 7: Observability, Evals, & Validation"
description: Build production-ready agents with quality checks, monitoring, and validation
---

You've built agents that can reason, remember, and collaborate. Now it's time to make them reliable and safe for production use.

## The Production Reality

When [Salesforce deployed their Agentforce AI agents](https://www.hr-brew.com/stories/2025/03/04/salesforce-ai-agents-reskilling), they discovered that success required more than technology—it demanded comprehensive guardrails, systematic evaluation, and robust observability. According to [NIST's AI Risk Management framework](https://www.nist.gov/itl/ai-risk-management-framework), the primary operational risks in AI systems include hallucination, prompt injection, resource consumption, and compliance drift.

The gap between demo and production? Proper validation, quality checks, and comprehensive monitoring.

## Three Pillars for Production Agents

### Observability

Agentuity provides automatic OpenTelemetry integration with zero configuration. All agent executions, LLM calls, and storage operations are traced automatically and appear in the Sessions tab with timeline visualization.

Custom spans, structured logging, and child loggers give you deep visibility into agent behavior.

### Validation

Schema-based validation handles most cases automatically—input and output validation happens before and after your handler executes. Advanced patterns with Zod (refinements, transforms, cross-field validation) handle complex business rules.

### Quality Assurance

The evaluations system lets you define automated quality checks that run after agent execution. From simple confidence thresholds to complex LLM-as-judge patterns, evaluations ensure your agents meet production standards.

---

## Tutorial Steps

Each step demonstrates a core pattern for production-ready agents.

### Step 1: Basic Evaluations

<TutorialStep number={1} title="Basic Evaluations" estimatedTime="8 min">

Evaluations run automated quality checks after your agent executes. This step shows a simple binary pass/fail evaluation for a sentiment analyzer.

<CodeFromFiles snippets={[
  { path: "/examples/training/07-observability-evals-validation/step1-basic-evaluations/agent.ts", lang: "ts", title: "agent.ts" },
  { path: "/examples/training/07-observability-evals-validation/step1-basic-evaluations/route.ts", lang: "ts", title: "route.ts" }
]} />

**What this demonstrates:**
- Creating evaluations with `agent.createEval()`
- Binary pass/fail pattern
- Access to input and output in eval handler
- Metadata tracking for debugging
- Non-blocking execution (runs after response sent)

**Try it:**
1. Start DevMode: `agentuity dev`
2. Send text: `{"text": "This product is amazing!"}`
3. Check logs for eval results showing high confidence
4. Send ambiguous text: `{"text": "It's okay I guess"}`
5. Notice response returns immediately—eval runs after

> **Key Insight:** Evaluations run asynchronously after the response is sent using `waitUntil()`. They don't block your agent's response but provide crucial quality signals for monitoring and improvement.

</TutorialStep>

---

### Step 2: Advanced Evaluations

<TutorialStep number={2} title="Advanced Evaluations" estimatedTime="8 min">

Advanced evaluations use LLM-as-judge patterns for complex quality checks. This step shows a content moderator with score-based safety evaluation.

<CodeFromFiles snippets={[
  { path: "/examples/training/07-observability-evals-validation/step2-advanced-evals/agent.ts", lang: "ts", title: "agent.ts" },
  { path: "/examples/training/07-observability-evals-validation/step2-advanced-evals/route.ts", lang: "ts", title: "route.ts" }
]} />

**What this demonstrates:**
- Score-based evaluations (0-1 range)
- LLM-as-judge pattern with structured output
- Using `generateObject` within eval handlers
- Detailed metadata with reasoning
- Safety and compliance checking

**Try it:**
1. Send safe content: `{"content": "How do I reset my password?"}`
2. Check logs for high safety scores
3. Send potentially problematic content
4. Notice detailed reasoning in eval metadata

> **Key Insight:** LLM-as-judge evaluations use AI to assess quality, safety, or task completion. Return score-based results (0-1) for nuanced quality tracking, not just binary pass/fail.

</TutorialStep>

---

### Step 3: Event System

<TutorialStep number={3} title="Event System" estimatedTime="8 min">

The event system provides lifecycle hooks for monitoring agent execution. This step shows agent-level events for performance tracking.

<CodeFromFiles snippets={[
  { path: "/examples/training/07-observability-evals-validation/step3-event-system/agent.ts", lang: "ts", title: "agent.ts" },
  { path: "/examples/training/07-observability-evals-validation/step3-event-system/route.ts", lang: "ts", title: "route.ts" }
]} />

**What this demonstrates:**
- Agent-level event listeners (started, completed, errored)
- Performance monitoring with timing
- State access in event handlers
- Error tracking and logging
- Threshold-based alerting

**Try it:**
1. Send requests and watch execution logs
2. Notice timing information in completed events
3. Check for slow execution warnings
4. Try triggering errors to see error events

> **Key Insight:** Events execute sequentially during the agent lifecycle. Use them for monitoring, analytics, and alerting. For non-blocking work in event handlers, use `c.waitUntil()`.

</TutorialStep>

---

### Step 4: Structured Logging

<TutorialStep number={4} title="Structured Logging" estimatedTime="8 min">

Child loggers attach context automatically, making logs easier to filter and analyze. This step shows component-based logging in a data processor.

<CodeFromFiles snippets={[
  { path: "/examples/training/07-observability-evals-validation/step4-structured-logging/agent.ts", lang: "ts", title: "agent.ts" },
  { path: "/examples/training/07-observability-evals-validation/step4-structured-logging/route.ts", lang: "ts", title: "route.ts" }
]} />

**What this demonstrates:**
- Creating child loggers with `c.logger.child()`
- Component-based context propagation
- Different log levels (info, debug, warn)
- Structured logging with objects
- Filtering logs by component

**Try it:**
1. Send valid data: `{"data": ["a", "b", "c"], "userId": "123"}`
2. Send data with empty strings to trigger warnings
3. Filter DevMode logs by `component:validation`
4. Notice how component context appears in every log

> **Key Insight:** Child loggers propagate context automatically. Create them once at the start of different workflow phases, and all subsequent logs include that context—making debugging and filtering much easier.

</TutorialStep>

---

### Step 5: Advanced Validation

<TutorialStep number={5} title="Advanced Validation" estimatedTime="8 min">

Schema validation goes beyond simple type checking. This step shows transforms, refinements, and cross-field validation for user input.

<CodeFromFiles snippets={[
  { path: "/examples/training/07-observability-evals-validation/step5-advanced-validation/agent.ts", lang: "ts", title: "agent.ts" },
  { path: "/examples/training/07-observability-evals-validation/step5-advanced-validation/route.ts", lang: "ts", title: "route.ts" }
]} />

**What this demonstrates:**
- String transformations (toLowerCase, trim)
- Type transformations (string to number)
- Custom refinements for business rules
- Cross-field validation between properties
- Automatic validation before handler execution

**Try it:**
1. Send input with uppercase email: `{"email": "USER@EXAMPLE.COM", ...}`
2. Notice email automatically lowercased
3. Try invalid password (no uppercase or number)
4. Try invalid dates (endDate before startDate)
5. See validation errors before handler runs

> **Key Insight:** Transformations run during validation, modifying data before your handler receives it. Refinements add custom business rules beyond basic type checking. Both execute automatically—your handler only sees valid, transformed data.

</TutorialStep>

---

### Step 6: Custom Spans & Tracing

<TutorialStep number={6} title="Custom Spans & Tracing" estimatedTime="8 min">

OpenTelemetry integration provides automatic tracing. Custom spans let you track specific operations within your agent's business logic.

<CodeFromFiles snippets={[
  { path: "/examples/training/07-observability-evals-validation/step6-custom-spans/agent.ts", lang: "ts", title: "agent.ts" },
  { path: "/examples/training/07-observability-evals-validation/step6-custom-spans/route.ts", lang: "ts", title: "route.ts" }
]} />

**What this demonstrates:**
- Creating custom spans with `c.tracer.startActiveSpan()`
- Nested spans (parent-child hierarchy)
- Setting span attributes for metadata
- Adding events to mark milestones
- Recording exceptions and setting status
- Always calling `span.end()` in finally blocks

**Try it:**
1. Send data batch: `{"data": ["item1", "item2", ""]}`
2. Open DevMode Sessions tab
3. View span timeline showing nested execution
4. Check span attributes and events
5. See parent-child span relationships

> **Key Insight:** Spans create hierarchical traces of your agent's execution. Use attributes for metadata, events for milestones, and always end spans in finally blocks. The Sessions tab visualizes the complete execution timeline.

</TutorialStep>

---

## Advanced Patterns

The tutorial steps cover core observability, evaluation, and validation concepts. Here are additional patterns for complex production scenarios.

### Content Moderation with LLM Jury

For applications requiring compliance validation (finance, healthcare, customer support), combine structural validation with AI-powered content evaluation:

```typescript
// First: validate structure
const result = InputSchema.safeParse(data);
if (!result.success) return { error: result.error };

// Second: evaluate content appropriateness
const evaluation = await generateObject({
  model: anthropic('claude-sonnet-4-5'),
  schema: ContentEvaluationSchema,
  prompt: `Evaluate for compliance: "${result.data.query}"`
});

if (!evaluation.object.approved) {
  return { error: 'Content policy violation' };
}

// Third: process validated and approved data
return await processQuery(result.data);
```

This two-stage validation ensures both structural validity and content appropriateness before processing.

### External API Validation

When working with external APIs, defensive validation prevents downstream errors from malformed responses:

```typescript
const rawData = await fetchExternalAPI();

// Validate with Zod
const parseResult = ExternalDataSchema.safeParse(rawData);

if (parseResult.success) {
  await processData(parseResult.data);
} else {
  c.logger.warn('Invalid external data', {
    errors: parseResult.error.issues
  });
  // Fallback or error handling
}
```

Always validate external data—don't trust that APIs return what their documentation claims.

### Metrics and Golden Datasets

Build evaluation datasets covering critical scenarios:
- Input examples representing edge cases
- Expected behavior for each scenario
- Success criteria for validation

Track metrics that matter for your domain:
- Accuracy (compare against test cases)
- Compliance (check output validation pass rates)
- Performance (track timing in telemetry)
- Cost (monitor token usage per request)

Use evaluations to measure these metrics automatically and catch regressions as you refine your agents.

---

## Key Takeaways

By the end of this module, you should understand:

1. **Evaluations:**
   - Create automated quality checks with `agent.createEval()`
   - Binary pass/fail or score-based (0-1) patterns
   - LLM-as-judge for complex quality assessment
   - Non-blocking execution via `waitUntil()`

2. **Events:**
   - Lifecycle hooks at agent level (started, completed, errored)
   - Performance monitoring and error tracking
   - State access in event handlers
   - Sequential execution for reliability

3. **Structured Logging:**
   - Child loggers with automatic context propagation
   - Component-based organization
   - Different log levels for appropriate visibility
   - Filtering and searching in production

4. **Advanced Validation:**
   - Automatic schema validation (input/output)
   - Transformations modify data during validation
   - Refinements add custom business rules
   - Cross-field validation for complex constraints

5. **Custom Spans:**
   - OpenTelemetry integration (automatic, zero config)
   - Custom spans for business logic tracing
   - Nested spans for hierarchical operations
   - Attributes, events, and exception recording

All observability features are non-blocking and type-safe, designed for production use from day one.

---

## What's Next?

For deeper dives into these concepts, see:
- [Evaluations Guide](/v1/Guides/evaluations)
- [Events Guide](/v1/Guides/events)
- [Logging Guide](/v1/Guides/logging)
- [Schema Validation Guide](/v1/Guides/schema-validation)
