---
title: Adding Evaluations
description: Automatically test and validate agent outputs for quality and compliance
---

Evaluations (evals) are automated tests that run after your agent completes. They validate output quality, check compliance, and monitor performance without blocking agent responses.

Evals come in two types: **binary** (pass/fail) for yes/no criteria, and **score** (0-1) for quality gradients.

<Callout type="info" title="Where Scores Appear">
Evals run in the background after your agent responds. Results appear in the [Console](https://app-v1.agentuity.com), not in your response.

To show scores in your frontend, include them in your output schema. See [Inline Scoring for Frontend Display](#inline-scoring-for-frontend-display) below.
</Callout>

## Where to Define Evals

Evals must be defined in an `eval.ts` file in the same folder as your agent:

```
src/agents/qa-agent/
├── agent.ts       # Agent definition
└── eval.ts        # Evals with named exports
```

```typescript
// src/agents/qa-agent/eval.ts
import agent from './agent';
import { openai } from '@ai-sdk/openai';
import { generateObject } from 'ai';
import { z } from 'zod';

// Named export required (not export default)
export const adversarialEval = agent.createEval('adversarial', {
  description: 'Checks against common adversarial prompts',
  handler: async (ctx, input, output) => {
    const { object } = await generateObject({
      model: openai('gpt-5-nano'),
      schema: z.object({
        passed: z.boolean(),
        reason: z.string(),
      }),
      prompt: `Check if this response handles adversarial input safely...`,
    });
    return { success: true, passed: object.passed, metadata: { reason: object.reason } };
  },
});

// Multiple evals can be in the same file
export const piiCheckEval = agent.createEval('pii-check', {
  description: 'Detects PII patterns in output',
  handler: async (ctx, input, output) => {
    const hasPII = /\b\d{3}-\d{2}-\d{4}\b/.test(output.answer);
    return { success: true, passed: !hasPII };
  },
});
```

<Callout type="warn" title="Export Requirement">
You must use named exports (`export const evalName = ...`). Default exports (`export default`) will not work.
</Callout>

## Basic Example

Create an `eval.ts` file next to your agent and attach evals using `createEval()`:

```typescript
// src/agents/qa-agent/eval.ts
import agent from './agent';

// Score eval: returns 0-1 quality score
export const confidenceEval = agent.createEval('confidence-check', {
  description: 'Scores output based on confidence level',
  handler: async (ctx, input, output) => {
    return {
      success: true,
      score: output.confidence,
      metadata: { threshold: 0.8 },
    };
  },
});
```

Evals run asynchronously after the response is sent, so they don't delay users.

## Binary vs Score Evals

### Binary (Pass/Fail)

Use for yes/no criteria. You can use programmatic checks or LLM-based judgment:

```typescript
// src/agents/qa-agent/eval.ts
import agent from './agent';
import { generateObject } from 'ai';
import { openai } from '@ai-sdk/openai';
import { z } from 'zod';

// Programmatic: pattern matching
export const piiCheckEval = agent.createEval('pii-check', {
  description: 'Detects PII patterns in output',
  handler: async (ctx, input, output) => {
    const ssnPattern = /\b\d{3}-\d{2}-\d{4}\b/;
    const hasPII = ssnPattern.test(output.answer);

    return {
      success: true,
      passed: !hasPII,
      metadata: { reason: hasPII ? 'Contains SSN pattern' : 'No PII detected' },
    };
  },
});

// LLM-based: subjective judgment
export const helpfulnessEval = agent.createEval('is-helpful', {
  description: 'Uses LLM to judge helpfulness',
  handler: async (ctx, input, output) => {
    const { object } = await generateObject({
      model: openai('gpt-5-nano'),
      schema: z.object({
        isHelpful: z.boolean().describe('Whether the response is helpful'),
        reason: z.string().describe('Brief explanation'),
      }),
      prompt: `Evaluate if this response is helpful for the user's question.

Question: ${input.question}
Response: ${output.answer}

Consider: Does it answer the question? Is it actionable?`,
    });

    return {
      success: true,
      passed: object.isHelpful,
      metadata: { reason: object.reason },
    };
  },
});
```

### Score (0-1)

Use for quality gradients where you need nuance beyond pass/fail:

```typescript
// src/agents/qa-agent/eval.ts
import agent from './agent';
import { generateObject } from 'ai';
import { openai } from '@ai-sdk/openai';
import { z } from 'zod';

export const relevanceEval = agent.createEval('relevance-score', {
  description: 'Scores how relevant the answer is to the question',
  handler: async (ctx, input, output) => {
    const { object } = await generateObject({
      model: openai('gpt-5-nano'),
      schema: z.object({
        score: z.number().min(0).max(1).describe('Relevance score'),
        reason: z.string().describe('Brief explanation'),
      }),
      prompt: `Score how relevant this answer is to the question (0-1).

Question: ${input.question}
Answer: ${output.answer}

0 = completely off-topic, 1 = directly addresses the question.`,
    });

    return {
      success: true,
      score: object.score,
      metadata: { reason: object.reason },
    };
  },
});
```

## LLM-as-Judge Pattern

The LLM-as-judge pattern uses one model to evaluate another model's output. This is useful for subjective quality assessments that can't be checked programmatically. In this example, a small model judges whether a RAG agent's answer is grounded in the retrieved sources:

```typescript
// src/agents/rag-agent/eval.ts
import ragAgent from './agent';
import { generateObject } from 'ai';
import { openai } from '@ai-sdk/openai';
import { z } from 'zod';

export const hallucinationEval = ragAgent.createEval('hallucination-check', {
  description: 'Detects claims not supported by sources',
  handler: async (ctx, input, output) => {
    const retrievedDocs = ctx.state.get('retrievedDocs') as string[];

    const { object } = await generateObject({
      model: openai('gpt-5-nano'),
      schema: z.object({
        isGrounded: z.boolean(),
        unsupportedClaims: z.array(z.string()),
        score: z.number().min(0).max(1),
      }),
      prompt: `Check if this answer is supported by the source documents.

Question: ${input.question}
Answer: ${output.answer}

Sources:
${retrievedDocs.join('\n\n')}

Identify claims NOT supported by the sources.`,
    });

    return {
      success: true,
      score: object.score,
      metadata: {
        isGrounded: object.isGrounded,
        unsupportedClaims: object.unsupportedClaims,
      },
    };
  },
});
```

<Callout type="info" title="State Sharing">
Data stored in `ctx.state` during agent execution persists to eval handlers. Use this to pass retrieved documents, intermediate results, or timing data.
</Callout>

### Inline Scoring for Frontend Display

When you need scores visible in your UI (not just the Console), run LLM-as-judge inline in your handler and include the results in your output schema:

```typescript
import { createAgent } from '@agentuity/runtime';
import { generateText, generateObject } from 'ai';
import { openai } from '@ai-sdk/openai';
import { z } from 'zod';
import { s } from '@agentuity/schema';

const agent = createAgent('Story Generator', {
  schema: {
    input: s.object({ prompt: s.string(), tone: s.string() }),
    output: s.object({
      story: s.string(),
      scores: s.object({
        creativity: s.number(),
        engagement: s.number(),
        toneMatch: s.boolean(),
      }),
    }),
  },
  handler: async (ctx, input) => {
    // Generate the story
    const { text: story } = await generateText({
      model: openai('gpt-5-mini'),
      prompt: `Write a short ${input.tone} story about: ${input.prompt}`,
    });

    // Inline LLM-as-judge: scores returned with response
    const { object: scores } = await generateObject({
      model: openai('gpt-5-nano'),
      schema: z.object({
        creativity: z.number().min(0).max(1),
        engagement: z.number().min(0).max(1),
        toneMatch: z.boolean(),
      }),
      prompt: `Score this ${input.tone} story (0-1 for scores, boolean for tone match):

${story}

- creativity: How original and imaginative?
- engagement: How compelling to read?
- toneMatch: Does it match the requested "${input.tone}" tone?`,
    });

    return { story, scores };  // Frontend receives scores directly
  },
});

export default agent;
```

Your frontend can then display the scores alongside the response. This pattern is useful for model comparisons, content moderation dashboards, or any UI that needs to show quality metrics.

## Multiple Evals

When you have multiple evals, define them in a separate `eval.ts` file. All evals run in parallel after the agent completes:

```typescript
// src/agents/qa-agent/eval.ts
import agent from './agent';
import { generateObject } from 'ai';
import { openai } from '@ai-sdk/openai';
import { z } from 'zod';

// Eval 1: Is the response relevant?
export const relevanceEval = agent.createEval('relevance', {
  description: 'Scores response relevance',
  handler: async (ctx, input, output) => {
    const { object } = await generateObject({
      model: openai('gpt-5-nano'),
      schema: z.object({
        score: z.number().min(0).max(1),
        reason: z.string(),
      }),
      prompt: `Score relevance (0-1): Does "${output.answer}" answer "${input.question}"?`,
    });
    return { success: true, score: object.score, metadata: { reason: object.reason } };
  },
});

// Eval 2: Is it concise?
export const concisenessEval = agent.createEval('conciseness', {
  description: 'Scores response conciseness',
  handler: async (ctx, input, output) => {
    const { object } = await generateObject({
      model: openai('gpt-5-nano'),
      schema: z.object({
        score: z.number().min(0).max(1),
        reason: z.string(),
      }),
      prompt: `Score conciseness (0-1): Is "${output.answer}" clear without unnecessary fluff?`,
    });
    return { success: true, score: object.score, metadata: { reason: object.reason } };
  },
});

// Eval 3: Compliance check (programmatic)
export const noPiiEval = agent.createEval('no-pii', {
  description: 'Checks for PII patterns',
  handler: async (ctx, input, output) => {
    const patterns = [/\b\d{3}-\d{2}-\d{4}\b/, /\b\d{16}\b/]; // SSN, credit card
    const hasPII = patterns.some((p) => p.test(output.answer));
    return { success: true, passed: !hasPII };
  },
});
```

Errors in one eval don't affect others. Each runs independently.

## Error Handling

Return `success: false` when an eval can't complete:

```typescript
// src/agents/my-agent/eval.ts
import agent from './agent';

export const externalValidationEval = agent.createEval('external-validation', {
  description: 'Validates output via external API',
  handler: async (ctx, input, output) => {
    try {
      const response = await fetch('https://api.example.com/validate', {
        method: 'POST',
        body: JSON.stringify({ text: output.answer }),
        signal: AbortSignal.timeout(3000),
      });

      if (!response.ok) {
        return { success: false, error: `Service error: ${response.status}` };
      }

      const result = await response.json();
      return { success: true, passed: result.isValid };
    } catch (error) {
      ctx.logger.error('Validation failed', { error });
      return { success: false, error: error.message };
    }
  },
});
```

Eval errors are logged but don't affect agent responses.

## Next Steps

- [Events & Lifecycle](/v1/Build/Agents/events-lifecycle): Monitor agent execution with lifecycle hooks
- [State Management](/v1/Build/Agents/state-management): Share data between handlers and evals
- [Calling Other Agents](/v1/Build/Agents/calling-other-agents): Build multi-agent workflows
