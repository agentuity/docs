---
title: Agent Streaming
description: How to use streaming in your agents
---

> **Streaming lets your users read the response before the AI finishes thinking.**  Nothing feels faster than already happening.

## Why Streaming?

- **Latency hiding** by showing results instantly instead of after the whole response is ready.
- **Large inputs and outputs** without hitting payload limits.
- **Agent chains** can forward chunks to the next agent as soon as they arrive.
- **Snappier UX** so users see progress in milliseconds instead of waiting for the full payload.
- **Resource efficiency** by not holding entire responses in memory; chunks flow straight through.
- **Composable pipelines** by allowing agents, functions, and external services to hand off work in a continuous stream.

**A simple visualization of the difference between traditional request/response and streaming:**

```bash
┌─────────────────────────── traditional request/response ───────────────────────────────────┐
|  client waiting ...   ██████████████████████████████████████████  full payload   display   |
└────────────────────────────────────────────────────────────────────────────────────────────┘

┌─────────────────────────── streaming request/response ─────────────────────────────────────┐
|  c  l  i  e  n  t  r  e  a  d  s  c  h  u  n  k  1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 … |
└────────────────────────────────────────────────────────────────────────────────────────────┘
```

### Real-World Use Cases

- **Live chat / customer support.** Stream the assistant's words as they are generated for a more natural feel.
- **Speech-to-text.** Pipe microphone audio into a transcription agent and forward captions to the UI in real time.
- **Streaming search results.** Show the first relevant hits immediately while the rest are still processing.
- **Agent chains.** One agent can translate, the next can summarize, the third can analyze – all in a single flowing stream.

## How Streaming Works in Agentuity

Agentuity provides multiple approaches for streaming data:

1. **High-level Streaming:** `resp.stream(source)` – where `source` can be:
   - An async iterator (e.g. OpenAI SDK stream)
   - A ReadableStream
   - Another agent's stream
2. **Low-level Stream Control:** `context.stream.create(name, props)` – create and manage server-side streams directly
3. **Inbound:** `await request.data.stream()` – consume the client's incoming stream.

Under the hood Agentuity handles the details of the streaming input and output for you.

### OpenAI Streaming Example

In this example, we use the OpenAI SDK to stream the response from the OpenAI API back to the caller.

<CodeExample js={`import type { AgentRequest, AgentResponse, AgentContext } from "@agentuity/sdk";
import { streamText } from "ai";
import { openai } from "@ai-sdk/openai";

export default async function Agent(
  req: AgentRequest,
  resp: AgentResponse,
  ctx: AgentContext,
) {
  const { textStream } = streamText({
    model: openai("gpt-4o"),
    prompt: "Invent a new holiday and describe its traditions.",
  });

  return resp.stream(textStream, 'text/plain');
}`} py={`from openai import OpenAI
from agentuity import AgentRequest, AgentResponse, AgentContext

client = OpenAI()

async def run(request: AgentRequest, response: AgentResponse, context: AgentContext):
    chat_completion = client.chat.completions.create(
        messages=[
            {"role": "system", "content": "You are a friendly assistant!"},
            {"role": "user", "content": (await request.data.text()) or "Why is the sky blue?"},
        ],
        model="gpt-4o",
        stream=True,
    )
    return response.stream(chat_completion, lambda chunk: chunk.choices[0].delta.content)
`} />

### Structured Object Streaming with Vercel AI SDK

The stream method now supports transformer functions that can filter and transform stream items. This is particularly useful when working with structured data from AI SDKs like Vercel AI SDK's `streamObject`.

<CodeExample js={`import type { AgentRequest, AgentResponse, AgentContext } from "@agentuity/sdk";
import { openai } from '@ai-sdk/openai';
import { streamObject } from 'ai';
import { z } from 'zod';

export default async function Agent(
  req: AgentRequest,
  resp: AgentResponse,
  ctx: AgentContext,
) {
  const { elementStream } = streamObject({
    model: openai('gpt-4o'),
    output: 'array',
    schema: z.object({
      name: z.string(),
      class: z
        .string()
        .describe('Character class, e.g. warrior, mage, or thief.'),
      description: z.string(),
    }),
    prompt: 'Generate 3 hero descriptions for a fantasy role playing game.',
  });

  return resp.stream(elementStream);
}`} />

The SDK automatically detects object streams and converts them to JSON newline format with the appropriate `application/json` content type.

### Stream Transformers

You can provide an optional `transformer` parameter to `resp.stream()` to filter and transform stream data before it's sent to the client:

<CodeExample js={`import type { AgentRequest, AgentResponse, AgentContext } from "@agentuity/sdk";

export default async function Agent(
  req: AgentRequest,
  resp: AgentResponse,
  ctx: AgentContext,
) {
  // Get stream from another source
  const dataStream = getDataStream();

  // Transform and filter items
  const transformer = (item: any) => {
    // Filter out items (return null/undefined to skip)
    if (!item.active) return null;

    // Transform the item
    return {
      id: item.id,
      name: item.name.toUpperCase(),
      timestamp: Date.now()
    };
  };

  // Optional parameters can be omitted by passing undefined or left out entirely
  return resp.stream(dataStream, 'application/json', undefined, transformer);
}`} />

You can also use generator functions for more complex transformations:

<CodeExample js={`// Generator transformer that can yield multiple items or filter
function* transformer(item: any) {
  if (item.type === 'batch') {
    // Yield multiple items from a batch
    for (const subItem of item.items) {
      yield { ...subItem, processed: true };
    }
  } else if (item.valid) {
    // Yield single transformed item
    yield { ...item, enhanced: true };
  }
  // Return nothing to filter out invalid items
}`} />

### Agent-to-Agent Streaming

In this example, we use the Agentuity SDK to stream the response from one agent to another.

```ts
import type { AgentRequest, AgentResponse, AgentContext } from "@agentuity/sdk";

export default async function Agent(
  req: AgentRequest,
  resp: AgentResponse,
  ctx: AgentContext,
) {
  // [1] Call another agent
  const expert = await ctx.getAgent({ name: "HistoryExpert" });
  const expertResp = await expert.run({ prompt: "What engine did a P-51D Mustang use?" });

  // [2] Grab its stream
  const stream = await expertResp.data.stream();

  // [3] Pipe straight through
  return resp.stream(stream);
}
```

Chain as many agents as you like; each one can inspect, transform, or just relay the chunks.

## Low-Level Stream Control

For advanced use cases, you can use `context.stream.create()` to create and manage streams directly. This gives you fine-grained control over stream creation, data flow, and background processing.

### Creating Streams with `context.stream.create`

The `context.stream.create()` method creates a named, writable stream that returns immediately, allowing for non-blocking operation and background data processing.

When you have a ReadableStream source (like an LLM response, file stream, or another agent's output), use `pipeTo()` to efficiently pipe data to your created stream:

```ts
import type { AgentRequest, AgentResponse, AgentContext } from "@agentuity/sdk";
import { openai } from '@ai-sdk/openai';
import { streamText } from 'ai';

export default async function Agent(
  req: AgentRequest,
  resp: AgentResponse,
  ctx: AgentContext,
) {
  const { prompt } = await req.data.json();

  // Create a stream immediately - this returns right away
  const stream = await ctx.stream.create('llm-response', {
    contentType: 'text/plain',
    metadata: {
      requestId: req.id,
      type: 'llm-generation',
      model: 'gpt-4o',
      userId: req.headers.get('user-id')
    }
  });

  // Use waitUntil to handle streaming in the background
  ctx.waitUntil(async () => {
    const { textStream } = streamText({
      model: openai('gpt-4o'),
      prompt
    });

    // Pipe the LLM stream to our created stream
    await textStream.pipeTo(stream);
  });

  // Return stream information immediately
  return resp.json({
    streamId: stream.id,
    streamUrl: stream.url,
    status: 'streaming'
  });
}
```

### Manual Stream Writing

For chunk-by-chunk control over stream data (such as progress updates or custom formatting), use the `getWriter()` pattern. Since `Stream` extends `WritableStream`, you need to get a writer first.

**Type Flexibility:** The SDK automatically handles type conversion and encoding. You can pass different data types directly to `writer.write()`:

```typescript
// All of these work:
await writer.write('Hello World\n');                      // String - auto-encoded to UTF-8
await writer.write({ status: 'processing' });             // Object - auto-serialized to JSON
await writer.write(new Uint8Array([72, 101, 108, 108]));  // Binary - passed through
```

The SDK handles JSON serialization and UTF-8 encoding as needed.

**Note:** While the SDK auto-serializes objects to JSON, it doesn't include line terminators. When streaming multiple JSON objects (such as NDJSON - newline-delimited JSON), you'll need to manually add delimiters between objects. See the example below for a common pattern.

```ts
export default async function Agent(
  req: AgentRequest,
  resp: AgentResponse,
  ctx: AgentContext,
) {
  const stream = await ctx.stream.create('progress-updates', {
    contentType: 'application/json',
    metadata: { type: 'progress-tracking' }
  });

  ctx.waitUntil(async () => {
    // Get a writer from the WritableStream
    const writer = stream.getWriter();

    try {
      // Send progress updates
      const steps = ['Initializing', 'Processing', 'Analyzing', 'Finalizing'];

      for (let i = 0; i < steps.length; i++) {
        const progressData = {
          step: i + 1,
          total: steps.length,
          message: steps[i],
          progress: ((i + 1) / steps.length) * 100,
          timestamp: new Date().toISOString()
        };

        // Streaming multiple JSON objects line-by-line (NDJSON pattern)
        await writer.write(JSON.stringify(progressData) + '\n');

        // Simulate work
        await new Promise(resolve => setTimeout(resolve, 1000));
      }
    } finally {
      // Always release the lock and close the stream
      writer.releaseLock();
      await stream.close();
    }
  });

  return resp.json({
    streamId: stream.id,
    streamUrl: stream.url
  });
}
```

### Reading from Streams with `getReader()`

The `Stream` object provides a `getReader()` method that returns a `ReadableStream<Uint8Array>`. This is useful for consuming streams you've created internally:

```ts
export default async function Agent(
  req: AgentRequest,
  resp: AgentResponse,
  ctx: AgentContext,
) {
  const stream = await ctx.stream.create('internal-stream', {
    contentType: 'text/plain'
  });

  // Write data in the background
  ctx.waitUntil(async () => {
    const writer = stream.getWriter();
    try {
      await writer.write(new TextEncoder().encode('Processing data...\n'));
      // ... more processing ...
      await writer.write(new TextEncoder().encode('Complete!\n'));
    } finally {
      writer.releaseLock();
      await stream.close();
    }
  });

  // Get a ReadableStream from the stream's URL and forward it
  const readable = stream.getReader();
  return resp.stream(readable, 'text/plain');
}
```

**Note:** The `getReader()` method fetches data from the stream's URL and will block until data starts being written to the stream.

### Stream as Direct Response

You can return a stream directly from your agent handler, which will automatically redirect clients to the stream URL:

```ts
export default async function Agent(
  req: AgentRequest,
  resp: AgentResponse,
  ctx: AgentContext,
) {
  const { prompt } = await req.data.json();

  const stream = await ctx.stream.create('direct-stream', {
    contentType: 'text/plain'
  });

  ctx.waitUntil(async () => {
    const { textStream } = streamText({
      model: openai('gpt-4o'),
      prompt
    });
    await textStream.pipeTo(stream);
  });

  // Return the stream directly - client gets redirected to stream URL
  return stream;
}
```

### Benefits of Low-Level Stream Control

- **Non-blocking**: Stream creation returns immediately, allowing instant responses
- **Background Processing**: Use `ctx.waitUntil()` to handle data streaming without blocking
- **Rich Metadata**: Associate custom metadata with streams for tracking and debugging
- **Direct Access**: Clients can access streams via direct URLs
- **Flexible Content Types**: Support any content type (text, JSON, binary, etc.)
- **Manual Control**: Write data chunks manually for complex streaming scenarios

### When to Use Each Pattern

| Your Use Case | Recommended Approach |
|---------------|---------------------|
| Streaming LLM responses in background | `pipeTo()` with `ctx.stream.create()` |
| Progress updates during long operations | `getWriter()` with `ctx.stream.create()` |
| Reading back from your created stream | `stream.getReader()` |
| Forwarding data directly to the client | `resp.stream()` |
| Chaining to another agent | `resp.stream(await agent.data.stream())` |

---

## Further Reading

- Blog Post: [Agents just want to have streams](https://agentuity.com/blog/agent-streaming)
- SDK Examples: [JavaScript](/SDKs/javascript/examples#openai-streaming-example) · [Python](/SDKs/python/examples#streaming-responses-from-openai)
- Streaming Video Demo: [Watch on YouTube](https://youtu.be/HN_ElBfsWtE)

