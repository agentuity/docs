---
title: "Module 5: Observability, Guardrails, & Evals"
description: Making agents reliable, safe, and production-ready
---

You've built agents that can think, remember, and collaborate. Now it's time to make them production-ready.

## The Reality of Production Agents

When [Salesforce deployed their Agentforce AI agents](https://www.hr-brew.com/stories/2025/03/04/salesforce-ai-agents-reskilling), they discovered that success required more than just technology - it demanded a comprehensive reskilling strategy for their 72,000+ employees. As their EVP of talent growth noted: "This rise of digital labor powered by AI agents is truly reshaping the way our businesses operate." The gap between demo and production? Proper guardrails, systematic evaluation, and comprehensive observability.

According to [NIST's AI Risk Management framework](https://www.nist.gov/itl/ai-risk-management-framework), the primary operational risks in AI systems include:
- **Hallucination**: Agents generating plausible but incorrect information
- **Prompt Injection**: Adversarial inputs manipulating agent behavior
- **Resource Consumption**: Uncontrolled usage leading to excessive costs
- **Compliance Drift**: Agents violating domain-specific regulations

## The Three Pillars of Production Agents

<Callout type="info">
Agentuity provides built-in OpenTelemetry integration with automatic instrumentation, giving you comprehensive observability out of the box.
</Callout>

### 1. Observability: Seeing Everything

Agentuity's key advantage is automatic OpenTelemetry integration with zero configuration:

- **What's Tracked**: LLM calls, tool invocations, storage operations, API calls
- **Observability Triad**: Logs (events), Metrics (measurements), Traces (request flow)
- **Console View**: Timeline visualization with color-coded spans
- **Zero Setup**: Works out of the box - no instrumentation code required

### 2. Guardrails: Setting Boundaries

Guardrails prevent agents from harmful actions while preserving autonomy and ensuring they stay in line with what they're supposed to do:

- **Input Validation**: Schema enforcement, content filtering, size limits
- **Rate Limiting**: Prevent abuse and control costs per user/session
- **Security**: Prompt injection defense ([WASP](https://arxiv.org/abs/2407.01593)), tool permissions
- **Domain Rules**: Compliance checks, output validation, custom constraints

### 3. Evaluation: Measuring Success

Systematic evaluation is critical for non-deterministic agents:

- **Real-World Benchmarks**: [Ï„-Bench](https://sierra.ai/blog/benchmarking-ai-agents) (dynamic agent interactions), [TheAgentCompany](https://metadesignsolutions.com/benchmarking-ai-agents-in-2025-top-tools-metrics-performance-testing-strategies/) (multi-step professional tasks)
- **Golden Datasets**: Domain-specific test cases with expected outcomes
- **Production Metrics**: Success rates, latency, cost per request, goal completion rates
- **A/B Testing**: Shadow deployments, gradual rollouts, real user feedback

## Observability with OpenTelemetry

Agentuity automatically tracks everything through OpenTelemetry:

### What's Tracked (No Code Required)
- **Agent executions**: Full request/response lifecycle
- **LLM calls**: Prompts, completions, token usage, latency
- **Storage operations**: KV gets/sets, vector searches
- **API calls**: External HTTP requests

View in the Agentuity console Sessions tab with color-coded timeline visualization.

### Using the Logger

<CodeExample py={`async def run(request: AgentRequest, response: AgentResponse, context: AgentContext):
    # Extract data from request
    data = await request.data.json()
    user_id = request.metadata.get("user_id")

    # Logs appear in Sessions view with trace context
    context.logger.info("Processing", {"user_id": user_id})

    try:
        result = await process(data)
        context.logger.info("Success", {"count": len(result)})
    except Exception as e:
        context.logger.error("Failed", {"error": str(e)})
        raise`} js={`const handler = async (request, response, context) => {
  // Extract data from request
  const data = await request.data.json();
  const userId = request.metadata.get('user_id');

  // Logs appear in Sessions view with trace context
  context.logger.info('Processing', { userId });

  try {
    const result = await process(data);
    context.logger.info('Success', { count: result.length });
  } catch (error) {
    context.logger.error('Failed', { error: error.message });
    throw error;
  }
};`} />

### Custom Spans for Your Own Operations

Track important operations with custom spans to understand performance and debug issues:

<CodeExample py={`from opentelemetry.trace import Status, StatusCode

async def run(request: AgentRequest, response: AgentResponse, context: AgentContext):
    payload = await request.data.json()
    query = payload.get("query", "")

    # Create a span for the entire validation flow
    async with context.tracer.start_as_current_span("validate-financial-query") as span:
        # Add context about this operation
        span.set_attribute("user.tier", "premium")
        span.set_attribute("query.type", "retirement")
        span.set_attribute("query.length", len(query))

        try:
            # Track validation steps
            span.add_event("validation-started")

            if has_prohibited_terms(query):
                span.add_event("validation-failed", {"reason": "prohibited-terms"})
                span.set_status(Status(StatusCode.ERROR, "Prohibited terms detected"))
                return response.json({"error": "Invalid query"})

            span.add_event("validation-passed")

            # Track LLM call separately
            async with context.tracer.start_as_current_span("generate-advice") as llm_span:
                llm_span.set_attribute("model", "gpt-4")
                advice = await generate_advice(query)
                llm_span.set_attribute("response.tokens", count_tokens(advice))

            span.set_status(Status(StatusCode.OK))
            return response.json({"advice": advice})

        except Exception as e:
            span.record_exception(e)
            span.set_status(Status(StatusCode.ERROR, str(e)))
            raise`} js={`import { SpanStatusCode } from '@opentelemetry/api';

const handler = async (request, response, context) => {
  const { query = '' } = await request.data.json();

  // Create a span for the entire validation flow
  return context.tracer.startActiveSpan('validate-financial-query', async (span) => {
    // Add context about this operation
    span.setAttribute('user.tier', 'premium');
    span.setAttribute('query.type', 'retirement');
    span.setAttribute('query.length', query.length);

    try {
      // Track validation steps
      span.addEvent('validation-started');

      if (hasProhibitedTerms(query)) {
        span.addEvent('validation-failed', { reason: 'prohibited-terms' });
        span.setStatus({ code: SpanStatusCode.ERROR, message: 'Prohibited terms detected' });
        return response.json({ error: 'Invalid query' });
      }

      span.addEvent('validation-passed');

      // Track LLM call separately
      const advice = await context.tracer.startActiveSpan('generate-advice', async (llmSpan) => {
        llmSpan.setAttribute('model', 'gpt-4');
        const result = await generateAdvice(query);
        llmSpan.setAttribute('response.tokens', countTokens(result));
        llmSpan.end();
        return result;
      });

      span.setStatus({ code: SpanStatusCode.OK });
      return response.json({ advice });

    } catch (error) {
      span.recordException(error);
      span.setStatus({ code: SpanStatusCode.ERROR, message: error.message });
      throw error;
    } finally {
      span.end();
    }
  });
};`} />

### Performance Best Practices

Follow these optimization patterns to ensure your agents run efficiently in production:

| Strategy | Implementation |
|----------|---------------|
| **Cache expensive operations** | Store LLM responses in KV with TTL to avoid repeated calls |
| **Use parallel operations** | `Promise.all()` (JS) or `asyncio.gather()` (Python) for concurrent tasks |
| **Fail fast** | Validate inputs early to avoid unnecessary processing |
| **Track token usage** | Add token counts as span attributes to monitor costs |
| **Set meaningful attributes** | Include user tier, request type, and other context for filtering |

### What You Get Out of the Box

<Callout type="info">
**Built-in Observability**: Agentuity provides automatic OpenTelemetry instrumentation with zero configuration required.
</Callout>

Everything is tracked automatically:
- **LLM calls**: Model, tokens, latency, and responses
- **Storage operations**: Every KV get/set, vector search, object store operation
- **API calls**: External service interactions and latencies
- **Custom spans**: Your business logic with meaningful attributes
- **Visual debugging**: Color-coded timeline in the console shows execution flow

<Callout type="info">
See the [Agent Telemetry Guide](/Guides/agent-telemetry) for advanced tracing and custom spans.
</Callout>

## Implementing Guardrails

Let's implement essential guardrails for production agent systems, using patterns from our Conference Concierge system:

### 1. Schema Validation with Zod & Pydantic

Runtime validation is critical for agents. TypeScript and Python types only exist at development time - at runtime, your data needs validation.

**Zod** (TypeScript) and **Pydantic** (Python) are popular validation libraries in their respective ecosystems. Zod provides TypeScript-first schema validation with static type inference, while Pydantic offers high-performance validation for Python with type hints integration. Both solve the same problem: ensuring runtime data matches your type expectations.

For detailed API references, see the [Zod](https://zod.dev) and [Pydantic](https://docs.pydantic.dev/latest/) documentation.

<CodeExample py={`from datetime import datetime
from pydantic import BaseModel, Field, ValidationError
from typing import Optional, Literal

class UserQuery(BaseModel):
    query: str = Field(min_length=1, max_length=1000)
    user_id: str
    portfolio_value: Optional[float] = Field(None, gt=0)  # Must be > 0 if provided

class ContentEvaluation(BaseModel):
    approved: bool
    reason: str
    category: Literal['safe', 'risky', 'prohibited']
    confidence: float = Field(ge=0.0, le=1.0)

async def run(request: AgentRequest, response: AgentResponse, context: AgentContext):
    # 1. Validate structure
    try:
        raw_data = await request.data.json()
        validated = UserQuery(**raw_data)
    except ValidationError as e:
        return response.json({
            'error': 'Invalid request',
            'details': e.errors()
        })
    
    # 2. Use LLM jury for content evaluation
    evaluation = await evaluate_with_jury(validated.query, context)
    if not evaluation.approved:
        return response.json({
            'error': 'Content policy violation',
            'reason': evaluation.reason
        })

    # 3. Process validated and approved data
    return await process_query(validated.query, validated.user_id)

async def evaluate_with_jury(query: str, context) -> ContentEvaluation:
    """Use LLM jury to evaluate content appropriateness."""
    jury_prompt = f'''
    Evaluate this financial query for compliance and safety:
    "{query}"

    Check for:
    - Prohibited financial advice (guaranteed returns, insider trading)
    - Inappropriate requests (illegal activities)
    - Scam indicators or harmful content

    Respond with your evaluation as structured data.
    '''

    # Use structured generation with Python anthropic SDK
    from anthropic import AsyncAnthropic
    import json

    client = AsyncAnthropic()
    response = await client.messages.create(
        model="claude-3-haiku-20240307",
        max_tokens=1000,
        messages=[{"role": "user", "content": jury_prompt}]
    )

    # Parse response and validate with Pydantic
    try:
        response_data = json.loads(response.content[0].text)
        evaluation = ContentEvaluation(**response_data)
    except (json.JSONDecodeError, ValidationError) as e:
        # Fallback to safe evaluation
        evaluation = ContentEvaluation(
            approved=False,
            reason="Failed to parse AI evaluation",
            category="prohibited",
            confidence=0.0
        )

    context.logger.info("Jury evaluation", {
        "approved": evaluation.approved,
        "category": evaluation.category,
        "confidence": evaluation.confidence
    })

    return evaluation`} js={`import { z } from 'zod';
import { generateObject } from 'ai';
import { anthropic } from '@ai-sdk/anthropic';

// Define clean schema for structure
const UserQuerySchema = z.object({
  query: z.string().min(1).max(1000),
  userId: z.string(),
  portfolioValue: z.number().positive().optional()  // Must be > 0 if provided
});

// Content evaluation schema for LLM jury
const ContentEvaluationSchema = z.object({
  approved: z.boolean(),
  reason: z.string(),
  category: z.enum(['safe', 'risky', 'prohibited']),
  confidence: z.number().min(0).max(1)
});

// TypeScript types from schemas
type UserQuery = z.infer<typeof UserQuerySchema>;
type ContentEvaluation = z.infer<typeof ContentEvaluationSchema>;

const handler = async (request, response, context) => {
  // 1. Validate structure
  const result = UserQuerySchema.safeParse(await request.data.json());

  if (!result.success) {
    return response.json({
      error: 'Invalid request',
      details: result.error.issues
    });
  }

  // 2. Use LLM jury for content evaluation
  const evaluation = await evaluateWithJury(result.data.query, context);
  if (!evaluation.approved) {
    return response.json({
      error: 'Content policy violation',
      reason: evaluation.reason
    });
  }

  // 3. Process validated and approved data
  const { query, userId } = result.data;
  return await processQuery(query, userId);
};

async function evaluateWithJury(query: string, context): Promise<ContentEvaluation> {
  const juryPrompt = \`
  Evaluate this financial query for compliance and safety:
  "\${query}"

  Check for:
  - Prohibited financial advice (guaranteed returns, insider trading)
  - Inappropriate requests (illegal activities)
  - Scam indicators or harmful content

  Respond with your evaluation as structured data.
  \`;

  const evaluation = await generateObject({
    model: anthropic('claude-3-haiku'),
    schema: ContentEvaluationSchema,
    prompt: juryPrompt
  });

  context.logger.info('Jury evaluation', {
    approved: evaluation.object.approved,
    category: evaluation.object.category,
    confidence: evaluation.object.confidence
  });

  return evaluation.object;
};`} />

<Callout type="info">
**Key Pattern**: Separate validation from domain rules. Schemas validate structure, your specific rules come after.
</Callout>


#### Real-World Example: Validating External API Responses

Here's how the AI News Digest agent validates Hacker News API responses:

<CodeExample py={`from datetime import datetime
from pydantic import BaseModel, ValidationError
from typing import Optional, List

# Schema for Hacker News story
class HNStory(BaseModel):
    id: int                      # Required by HN API
    title: str                   # What we display
    url: Optional[str] = None    # Available if needed

# Schema for our digest data
class DigestData(BaseModel):
    summary: str
    sources: List[str]
    article_count: int
    timestamp: str
    source: str

async def fetch_top_stories(ctx, count=5):
    stories = []
    story_ids = await fetch_story_ids()  # Get from HN API
    
    for story_id in story_ids[:count]:
        raw_data = await fetch_story_data(story_id)
        
        # Validate with Pydantic
        try:
            story = HNStory(**raw_data)
            stories.append(story.title)
        except ValidationError as e:
            ctx.logger.warn(f"Invalid story data for ID {story_id}: {e}")
            continue  # Skip invalid stories
    
    return stories

async def run(request: AgentRequest, response: AgentResponse, context: AgentContext):
    # Fetch and validate stories
    articles = await fetch_top_stories(context, 5)
    
    # Generate AI summary
    summary = await generate_summary(articles)
    
    # Create and validate digest
    try:
        digest = DigestData(
            summary=summary,
            sources=articles,
            article_count=len(articles),
            timestamp=datetime.now().isoformat(),
            source="Hacker News API"
        )
    except ValidationError as e:
        context.logger.error(f"Digest validation failed: {e}")
        return response.json({"error": "Failed to create digest"})
    
    # Store validated digest
    await context.kv.set("digest", "latest", digest.model_dump())
    
    return response.json(digest.model_dump())`} js={`import { z } from 'zod';

// Zod schemas for type-safe API responses
const HNStorySchema = z.object({
  id: z.number(),             // Required by HN API
  title: z.string(),          // What we display
  url: z.string().optional(), // Available if needed
});

const DigestDataSchema = z.object({
  summary: z.string(),
  sources: z.array(z.string()),
  articleCount: z.number(),
  timestamp: z.string(),
  source: z.string(),
});

async function fetchTopStories(ctx, count = 5) {
  const stories = [];
  const storyIds = await fetchStoryIds(); // Get from HN API
  
  for (const id of storyIds.slice(0, count)) {
    const rawData = await fetchStoryData(id);
    
    // Validate with Zod
    const parseResult = HNStorySchema.safeParse(rawData);
    if (parseResult.success) {
      stories.push(parseResult.data.title);
    } else {
      ctx.logger.warn('Invalid story data for ID ' + id);
      continue; // Skip invalid stories
    }
  }
  
  return stories;
}

const handler = async (request, response, context) => {
  // Fetch and validate stories
  const articles = await fetchTopStories(context, 5);
  
  // Generate AI summary
  const summary = await generateSummary(articles);
  
  // Create and validate digest with Zod
  const digestResult = DigestDataSchema.safeParse({
    summary,
    sources: articles,
    articleCount: articles.length,
    timestamp: new Date().toISOString(),
    source: 'Hacker News API'
  });
  
  if (!digestResult.success) {
    context.logger.error('Digest validation failed', digestResult.error);
    return response.json({ error: 'Failed to create digest' });
  }
  
  // Store validated digest
  await context.kv.set('digest', 'latest', digestResult.data);
  
  return response.json(digestResult.data);
};`} />

#### Using Schemas with AI-Generated Output

Schemas ensure structured output from AI models:

<CodeExample py={`from pydantic import BaseModel
from typing import Literal
from ai import generate_object
from anthropic import anthropic

class AgentIntent(BaseModel):
    agent_type: Literal['support', 'sales', 'technical']
    confidence: float  # 0.0 to 1.0

async def run(request: AgentRequest, response: AgentResponse, context: AgentContext):
    user_message = await request.data.text()
    
    # AI generates structured, validated output
    intent = await generate_object(
        model=anthropic("claude-3-7-sonnet"),
        schema=AgentIntent,  # Pydantic ensures structure
        prompt=user_message
    )
    
    # intent.object is validated and typed
    if intent.object.confidence > 0.8:
        agent = await context.get_agent(name=intent.object.agent_type)
        return await agent.run(request)`} js={`import { z } from 'zod';
import { generateObject } from 'ai';
import { anthropic } from '@ai-sdk/anthropic';

const IntentSchema = z.object({
  agentType: z.enum(['support', 'sales', 'technical']),
  confidence: z.number().min(0).max(1)
});

const handler = async (request, response, context) => {
  const userMessage = await request.data.text();
  
  // AI generates structured, validated output
  const intent = await generateObject({
    model: anthropic('claude-3-7-sonnet'),
    schema: IntentSchema,  // Zod ensures structure
    prompt: userMessage
  });
  
  // intent.object is validated and typed
  if (intent.object.confidence > 0.8) {
    const agent = await context.getAgent({ name: intent.object.agentType });
    return await agent.run(request);
  }
};`} />

### 3. Orchestrators as Natural Guardrails

Remember the orchestrator pattern from Module 4? Orchestrators are your first line of defense:

<CodeExample py={`from pydantic import BaseModel
from typing import Literal

class ValidatedIntent(BaseModel):
    category: Literal['technical', 'sales', 'support']
    confidence: float
    sensitive: bool = False
    
async def orchestrator_with_guardrails(request, response, context):
    user_prompt = await request.data.text()
    
    # 1. Input validation
    if len(user_prompt) > 1000:
        return response.json({"error": "Request too long"})
    
    # 2. Analyze intent with validation
    intent = await analyze_intent(user_prompt, schema=ValidatedIntent)
    
    # 3. Apply routing rules
    if intent.sensitive:
        # Route sensitive requests to specialized agent
        return response.handoff(
            {"name": "compliance-agent"},
            user_prompt,
            metadata={"flagged": True}
        )
    
    if intent.confidence < 0.7:
        # Low confidence = fallback to human
        return response.json({
            "message": "I'll connect you with a specialist",
            "transfer": "human"
        })
    
    # 4. Route to appropriate agent
    agent_map = {
        "technical": "tech-expert",
        "sales": "sales-agent",
        "support": "support-agent"
    }
    
    return response.handoff(
        {"name": agent_map[intent.category]},
        user_prompt
    )`} js={`import { z } from 'zod';

const ValidatedIntentSchema = z.object({
  category: z.enum(['technical', 'sales', 'support']),
  confidence: z.number(),
  sensitive: z.boolean().default(false)
});

const orchestratorWithGuardrails = async (request, response, context) => {
  const userPrompt = await request.data.text();
  
  // 1. Input validation
  if (userPrompt.length > 1000) {
    return response.json({ error: 'Request too long' });
  }
  
  // 2. Analyze intent with validation
  const { object: intent } = await analyzeIntent(userPrompt, {
    schema: ValidatedIntentSchema
  });
  
  // 3. Apply routing rules
  if (intent.sensitive) {
    // Route sensitive requests to specialized agent
    return response.handoff(
      { name: 'compliance-agent' },
      { 
        data: JSON.stringify({ query: userPrompt, flagged: true }),
        contentType: 'application/json'
      }
    );
  }
  
  if (intent.confidence < 0.7) {
    // Low confidence = fallback to human
    return response.json({
      message: "I'll connect you with a specialist",
      transfer: 'human'
    });
  }
  
  // 4. Route to appropriate agent
  const agentMap = {
    technical: 'tech-expert',
    sales: 'sales-agent',
    support: 'support-agent'
  };
  
  return response.handoff(
    { name: agentMap[intent.category] },
    { data: userPrompt, contentType: 'text/plain' }
  );
};`} />

**Why Orchestrators Make Great Guardrails:**
1. **Single validation point** - Check all inputs before routing
2. **Access control** - Decide who can access which agents
3. **Fallback handling** - Gracefully handle edge cases
4. **Audit trail** - Log all routing decisions
5. **Rate limiting** - Apply per-user limits before delegation

<Callout type="info">
See the complete [Conference Concierge implementation](https://github.com/agentuity/agent-AIEWF2025-concierge-template) for a production example of multi-agent routing with schema validation.
</Callout>

### 2. Rate Limiting

Prevent abuse and control costs:

<CodeExample py={`async def run(request: AgentRequest, response: AgentResponse, context: AgentContext):
    from datetime import datetime

    user_id = request.metadata.get("user_id")
    hour_key = f"rate_{user_id}_{datetime.now().hour}"
    
    # Check current usage
    usage = await context.kv.get("rate_limits", hour_key)
    count = await usage.data.json() if usage.exists else 0
    
    if count >= 100:  # 100 requests per hour
        return response.json({
            "error": "Rate limit exceeded",
            "retry_after": 3600
        })
    
    # Increment counter with TTL
    await context.kv.set("rate_limits", hour_key, count + 1, {"ttl": 3600})
    
    # Process request
    return await handle_request(request)`} js={`const handler: AgentHandler = async (request, response, context) => {
  const userId = request.metadata.get('user_id');
  const hourKey = 'rate_' + userId + '_' + new Date().getHours();
  
  // Check current usage
  const usage = await context.kv.get('rate_limits', hourKey);
  const count = usage.exists ? await usage.data.json() : 0;
  
  if (count >= 100) {  // 100 requests per hour
    return response.json({
      error: 'Rate limit exceeded',
      retry_after: 3600
    });
  }
  
  // Increment counter with TTL
  await context.kv.set('rate_limits', hourKey, count + 1, { ttl: 3600 });
  
  // Process request
  return await handleRequest(request);
};`} />

### 4. Domain-Specific Rules

Enforce rules specific to your use case:

<CodeExample py={`async def run(request: AgentRequest, response: AgentResponse, context: AgentContext):
    query = (await request.data.json()).get("query", "")

    # Domain-specific validation rules
    inappropriate_requests = ["hack", "illegal", "private information"]
    for term in inappropriate_requests:
        if term in query.lower():
            return response.json({
                "error": "Cannot assist with inappropriate requests",
                "reason": f"Request contains prohibited content: '{term}'"
            })

    # Length validation
    if len(query) > 1000:
        return response.json({
            "error": "Query too long",
            "max_length": 1000
        })

    # Process valid request
    result = await process_query(query, context)

    return response.json({
        "response": result,
        "processed_by": context.agent.name
    })`} js={`const handler: AgentHandler = async (request, response, context) => {
  const { query } = await request.data.json();

  // Domain-specific validation rules
  const inappropriateRequests = ['hack', 'illegal', 'private information'];
  for (const term of inappropriateRequests) {
    if (query.toLowerCase().includes(term)) {
      return response.json({
        error: 'Cannot assist with inappropriate requests',
        reason: \`Request contains prohibited content: '\${term}'\`
      });
    }
  }

  // Length validation
  if (query.length > 1000) {
    return response.json({
      error: 'Query too long',
      maxLength: 1000
    });
  }

  // Process valid request
  const result = await processQuery(query, context);

  return response.json({
    response: result,
    processedBy: context.agent.name
  });
};`} />

## Evaluation Strategies

<Callout type="info">
**Coming Soon**: Agentuity's integrated prompt library and automated evaluation suite will provide built-in prompt management and continuous evaluation workflows.
</Callout>

### Define Success Metrics

Choose metrics that matter for your domain:

| Metric | Example | How to Measure |
|--------|---------|----------------|
| **Accuracy** | Correct routing | Compare against test cases |
| **Compliance** | Appropriate responses | Check output validation |
| **Performance** | < 5s response | Track in telemetry |
| **Cost** | < $0.10/request | Monitor token usage |

### Build a Golden Dataset

Create test cases that cover your critical scenarios:

<CodeExample py={`# Golden dataset for concierge testing
test_cases = [
    {
        "id": "sf_local_question",
        "input": {"query": "Where's good coffee near Moscone Center?"},
        "expected": {
            "routed_to": "sf_local_guide",
            "contains": ["coffee", "moscone", "san francisco"],
            "confidence": 0.8
        }
    },
    {
        "id": "conference_question",
        "input": {"query": "What time is the keynote session?"},
        "expected": {
            "routed_to": "conference_expert",
            "contains": ["keynote", "session", "schedule"]
        }
    },
    {
        "id": "developer_question",
        "input": {"query": "How do I implement agent handoffs?"},
        "expected": {
            "routed_to": "dev_experience",
            "contains": ["handoff", "agent", "implementation"]
        }
    },
    {
        "id": "unclear_request",
        "input": {"query": "Help me with stuff"},
        "expected": {
            "fallback": True,
            "contains": ["clarify", "help with"]
        }
    }
]

async def evaluate_agent(agent_handler, test_cases):
    """Run evaluation suite and track success rate."""
    results = []
    
    for test in test_cases:
        # Run the agent
        response = await agent_handler(test["input"])
        
        # Check expectations
        passed = True
        if "fallback" in test["expected"]:
            # Check for fallback response pattern
            content = response.get("message", "").lower()
            passed = any(term in content for term in test["expected"].get("contains", []))
        elif "routed_to" in test["expected"]:
            # Check correct routing decision
            routed_agent = response.get("routed_to")
            passed = routed_agent == test["expected"]["routed_to"]

            # Also check response content
            content = response.get("response", "").lower()
            for term in test["expected"].get("contains", []):
                if term not in content:
                    passed = False
                    break
        
        results.append({
            "test_id": test["id"],
            "passed": passed,
            "response": response
        })
    
    # Calculate success rate
    success_rate = sum(1 for r in results if r["passed"]) / len(results)
    return {"success_rate": success_rate, "results": results}`} js={`// Golden dataset for concierge testing
const testCases = [
  {
    id: 'sf_local_question',
    input: { query: "Where's good coffee near Moscone Center?" },
    expected: {
      routedTo: 'sf_local_guide',
      contains: ['coffee', 'moscone', 'san francisco'],
      confidence: 0.8
    }
  },
  {
    id: 'conference_question',
    input: { query: 'What time is the keynote session?' },
    expected: {
      routedTo: 'conference_expert',
      contains: ['keynote', 'session', 'schedule']
    }
  },
  {
    id: 'developer_question',
    input: { query: 'How do I implement agent handoffs?' },
    expected: {
      routedTo: 'dev_experience',
      contains: ['handoff', 'agent', 'implementation']
    }
  },
  {
    id: 'unclear_request',
    input: { query: 'Help me with stuff' },
    expected: {
      fallback: true,
      contains: ['clarify', 'help with']
    }
  }
];

async function evaluateAgent(agentHandler, testCases) {
  const results = [];

  for (const test of testCases) {
    // Run the agent
    const response = await agentHandler(test.input);

    // Check expectations
    let passed = true;
    if (test.expected.fallback) {
      // Check for fallback response pattern
      const content = (response.message || '').toLowerCase();
      passed = test.expected.contains.some(term => content.includes(term));
    } else if (test.expected.routedTo) {
      // Check correct routing decision
      const routedAgent = response.routedTo;
      passed = routedAgent === test.expected.routedTo;

      // Also check response content
      const content = (response.response || '').toLowerCase();
      for (const term of test.expected.contains || []) {
        if (!content.includes(term)) {
          passed = false;
          break;
        }
      }
    }
    
    results.push({
      testId: test.id,
      passed,
      response
    });
  }
  
  // Calculate success rate
  const successRate = results.filter(r => r.passed).length / results.length;
  return { successRate, results };
}`} />


## Validation Patterns from Working Agent Systems

Instead of building a new agent from scratch, let's learn validation patterns from Module 4's Conference Concierge - a working system that shows useful validation techniques.

### Key Pattern: Structured LLM Output Validation

The concierge uses validation schemas to ensure AI generates reliable routing decisions:

<CodeExample py={`# Python - Pydantic validation for AI-generated routing decisions
from pydantic import BaseModel, ValidationError
from typing import Literal

class UserIntent(BaseModel):
    agent_type: Literal['sf_local_guide', 'conference_expert', 'dev_experience']
    confidence: float

# Use AI to analyze intent with validation
try:
    ai_response = await generate_structured_intent(user_message)
    intent = UserIntent(**ai_response)  # Pydantic validates structure
except ValidationError:
    # Safe fallback for invalid data
    intent = UserIntent(agent_type='dev_experience', confidence=0.5)

# Validated data enables confident decision making
if intent.confidence < 0.8:
    return response.json({
        "message": "I'll connect you with a human specialist"
    })`} js={`// TypeScript - Zod validation for AI-generated routing decisions
import { z } from 'zod';
import { generateObject } from 'ai';

const UserIntentSchema = z.object({
  agentType: z.enum(['sf_local_guide', 'conference_expert', 'dev_experience']),
  confidence: z.number().min(0).max(1)
});

// Validation ensures reliable AI output structure
const { object: userIntent } = await generateObject({
  model: anthropic('claude-3-5-sonnet-20241022'),
  schema: UserIntentSchema,  // Zod prevents invalid structures
  prompt: \`Analyze this user message and determine routing: "\${userMessage}"\`
});

// Validated data enables confident decision making
if (userIntent.confidence < 0.8) {
  return response.json({
    message: "I'll connect you with a human specialist"
  });
}`} />

### Key Pattern: Input Validation

Agents should validate all inputs before processing:

```python
# From Module 4: Pydantic validation patterns
from pydantic import BaseModel, Field, ValidationError
from typing import Literal

class ConversationRequest(BaseModel):
    message: str = Field(min_length=1, max_length=1000)
    session_id: str = Field(pattern=r'^[a-zA-Z0-9_-]+$')
    user_preferences: dict = Field(default_factory=dict)

async def concierge_handler(request: AgentRequest, response: AgentResponse, context: AgentContext):
    # Validate input structure with clear error handling
    try:
        data = await request.data.json()
        validated = ConversationRequest(**data)
    except ValidationError as e:
        context.logger.warning("Invalid input structure", {"errors": e.errors()})
        return response.json({
            "error": "Invalid request format",
            "details": [{"field": err["loc"][-1], "issue": err["msg"]} for err in e.errors()]
        })

    # Proceed with validated, type-safe data
    return await process_conversation(validated, context)
```

### Key Pattern: Domain-Specific Guardrails

The concierge applies context-aware validation rules:

```typescript
// Context-aware validation based on agent routing
async function validateAgentRouting(
  userIntent: UserIntent,
  conversationHistory: ConversationRecord,
  ctx: AgentContext
) {
  // Apply routing-specific validation rules
  if (userIntent.agentType === 'dev_experience') {
    // Technical questions require minimum context
    if (userIntent.context.length < 10) {
      ctx.logger.info('Insufficient context for technical routing');
      return { valid: false, reason: 'Please provide more detail about your question' };
    }
  }

  // Location questions need geographic context
  if (userIntent.agentType === 'sf_local_guide') {
    const hasLocationContext = /\b(san francisco|sf|bay area|moscone)\b/i.test(userIntent.context);
    if (!hasLocationContext) {
      return { valid: false, reason: 'Please specify your San Francisco location or area of interest' };
    }
  }

  return { valid: true };
}
```

### Pattern: Error Recovery and Fallbacks

Good validation includes graceful error recovery:

```python
# Defensive validation with multiple fallback strategies
async def validate_with_fallbacks(request_data: dict, context: AgentContext):
    # Strategy 1: Full validation
    try:
        validated = ConversationRequest(**request_data)
        return {"status": "validated", "data": validated}
    except ValidationError as primary_error:
        context.logger.info("Primary validation failed, trying fallbacks")

    # Strategy 2: Essential fields only
    try:
        essential_fields = {
            "message": request_data.get("message", ""),
            "session_id": request_data.get("session_id", f"fallback_{uuid.uuid4()}")
        }
        minimal = ConversationRequest(**essential_fields)
        context.logger.info("Using minimal validation fallback")
        return {"status": "fallback", "data": minimal}
    except ValidationError:
        context.logger.warning("All validation strategies failed")

    # Strategy 3: Safe defaults
    return {
        "status": "default",
        "data": ConversationRequest(
            message="I need help",
            session_id=f"emergency_{int(time.time())}"
        )
    }
```

## ðŸš€ Coming Soon: Integrated Prompts & Evaluations

Agentuity is working on prompt management and evaluation features:

### **Code-First Prompts**
- Version-controlled prompts in your codebase alongside agent logic
- Prompt templates with variable substitution

### **Automated Evaluations**
- Evals that run automatically after every agent session (configurable)
- Custom evaluation criteria specific to your domain and use cases
- Integration with your existing CI/CD pipeline

### **Experimentation Mode**
- Multi-model comparisons (e.g., GPT-5, Claude, Gemini) on identical prompts

<Callout type="info">
These features will provide better visibility into agent behavior and systematic ways to improve agent performance over time.
</Callout>

## Key Takeaways

- **Guardrails prevent failures** - Input validation, rate limiting, and domain rules protect your agents
- **Evaluation proves reliability** - Systematic testing with metrics that matter for your domain
- **Observability is automatic** - Agentuity provides OpenTelemetry integration for comprehensive monitoring
- **The console shows everything** - Sessions view with color-coded timeline visualization
- **Production readiness** requires all three pillars working together

## What's Next?

Now that you understand the fundamentals of production-ready agents, it's time to explore deployment. In the next module, we'll cover Agentuity's deployment environments - from local development through staging to production.

---

**Ready for Module 6?** [Deployment Environments](./06-deployment-environments)
